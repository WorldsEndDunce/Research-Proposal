\documentclass{sigchi}
\pagenumbering{arabic}

% Load basic packages
\usepackage{balance}       % to better equalize the last page
\usepackage{graphics}      % for EPS, load graphicx instead 
\usepackage[T1]{fontenc}   % for umlauts and other diaeresis
\usepackage{txfonts}
\usepackage{mathptmx}
\usepackage[pdflang={en-US},pdftex]{hyperref}
\usepackage{color}
\usepackage{booktabs}
\usepackage{textcomp}
\usepackage{soul}

% Some optional stuff you might like/need.
\usepackage{microtype}        % Improved Tracking and Kerning
% \usepackage[all]{hypcap}    % Fixes bug in hyperref caption linking
\usepackage{ccicons}          % Cite your images correctly!
% \usepackage[utf8]{inputenc} % for a UTF8 editor only

% If you want to use todo notes, marginpars etc. during creation of
% your draft document, you have to enable the "chi_draft" option for
% the document class. To do this, change the very first line to:
% "\documentclass[chi_draft]{sigchi}". You can then place todo notes
% by using the "\todo{...}"  command. Make sure to disable the draft
% option again before submitting your final document.
\usepackage{todonotes}

% Paper metadata
\def\plaintitle{Do Neural Networks Dream of an Efficient PNG Search Algorithm?}
\def\plainauthor{}
\def\emptyauthor{}
\def\plainkeywords{}
\def\plaingeneralterms{}

% llt: Define a global style for URLs, rather that the default one
\makeatletter
\def\url@leostyle{%
  \@ifundefined{selectfont}{
    \def\UrlFont{\sf}
  }{
    \def\UrlFont{\small\bf\ttfamily}
  }}
\makeatother
\urlstyle{leo}

% To make various LaTeX processors do the right thing with page size.
\def\pprw{8.5in}
\def\pprh{11in}
\special{papersize=\pprw,\pprh}
\setlength{\paperwidth}{\pprw}
\setlength{\paperheight}{\pprh}
\setlength{\pdfpagewidth}{\pprw}
\setlength{\pdfpageheight}{\pprh}

% Make sure hyperref comes last of your loaded packages, to give it a
% fighting chance of not being over-written, since its job is to
% redefine many LaTeX commands.
\definecolor{linkColor}{RGB}{6,125,233}
\hypersetup{%
  pdftitle={\plaintitle},
% Use \plainauthor for final version.
%  pdfauthor={\plainauthor},
  pdfauthor={\emptyauthor},
  pdfkeywords={\plainkeywords},
  pdfdisplaydoctitle=true, % For Accessibility
  bookmarksnumbered,
  pdfstartview={FitH},
  colorlinks,
  citecolor=black,
  filecolor=black,
  linkcolor=black,
  urlcolor=linkColor,
  breaklinks=true,
  hypertexnames=false
}

%% remove the copyright box
\makeatletter
\def\@copyrightspace{\relax}
\makeatother

% create a shortcut to typeset table headings
% \newcommand\tabhead[1]{\small\textbf{#1}}

% End of preamble. Here it comes the document.
\begin{document}

\title{\plaintitle}

\numberofauthors{1}
\author{%
  \alignauthor{Allison Tee\\
    \email{ateecup@stanford.edu}}\\
  }

\maketitle


%%%%%%%%%%%%%%%%%%%% TODO: Fill in these sections
\section{Summary}
Polychronous neuronal groups (PNGs) are bunches of neurons in the brain that fire in a specific pattern, and they represent memories. The amount of PNGs in a neural network scales combinatorially with the network size. Because there are more PNGs than neurons, it's hard to find these groups! Previous research in this field employed brute force methods, but they are slow and have limited accuracy. The objective of my research is to develop an efficient PNG-search algorithm that can find more PNGs with fewer measurements. The ability to find polychronous groups is relevant to fields like selective attention to memories, memory-related disorders, and reinforcement learning. I have prototyped an algorithm to find PNGs called "PP-Almost-Seq," which hypothesizes new polychronous groups based on an existing method to find neuron sequences called PP-Seq (short for the Neyman-Scott point process model for sequence detection). It has been tested on a network of 100 Izhikevich neurons simulated using the Brian 2 spiking neural network simulator in Python and works in simple cases with two implanted PNGs in the network. The next steps for my research include automating the loop between the simulation and the PP-Almost-Seq algorithm so that it can constantly detect new groups on its own and implementing a most likely order for candidate PNGs.
\section{Project Description}
By reverse-engineering the brain (one of the 14 grand engineering challenges \cite{grandchal}), it would be possible to build smarter computers that could assist with everyday life, fully understand how the brain makes humans human, and make machines that learn much more quickly and energy efficiently than current deep learning models. The brain is largely made up of neurons, so to achieve this overarching task, one would need to simulate realistic neural networks and develop algorithms to control the dynamics according to parameters for different neuron types. Specifically, the focus of this project is on "polychronous neuronal groups" (PNGs) that model how a collection of neurons can store a large quantity of memories with each group representing a particular memory \cite{pngintro}. Prominent neurocomputation researcher Eugene Izhikevich discovered that the number of PNGs is far greater than the number of neurons in a network, which causes the system to have an extremely high memory capacity. 

\hl{Although the combinatorially increasing number of PNGs allows for vast memory storage, it also causes the current brute-force algorithm that Izhikevich developed for discovering polychronous groups to be extremely inefficient and inaccurate.} It does not scale well with large networks, and the algorithm misses PNGs that do not start with a specific three-neuron firing pattern. Other challenges include the fact that probing a PNG rarely activates the entire group, network plasticity causes groups (and consequently memory) to degrade over time, and the network often experiences uncontrolled activity \cite{pngalgo}. 

The key contribution of this project is that instead of probing randomly like in the original algorithm, I develop a novel, more efficient algorithm to discover PNGs. A smart method to find polychronous groups is needed in order to study memories, especially given how the memory-storing PNGs are more densely populated than the neurons themselves. By developing this new algorithm, I improve the arsenal of existing neuroengineering control algorithms for studying the brain. The first step to develop this solution is to simulate neurons represented by differential equations. Equations with non-linear dynamics are difficult to model because they generally rely on numerical approximations. By using different and more precise numerical methods than previous researchers \cite{simtools}, one could create a more accurate simulation of neural networks. This is especially crucial for this problem because while the original algorithm searched through the neural population, the network was prone to seizures that could be alleviated by fine-tuning and optimizing the performance of the model. \hl{My solution for the problem is to introduce a new and improved probing algorithm to discover polychronous groups.} One of the advantages of the algorithm is collecting observations during development to mark likely PNGs in a network. The algorithm will be able to handle searching through larger neural networks and do so more accurately (for instance, identifying PNGs that do not start with three neurons).

To evaluate the programâ€™s ability to find PNGs, I compare how many measurements the algorithm needs of a neural population to discover PNGs relative to the brute-force search algorithm Izhikevich uses and how many PNGs are discovered. I also compare the sizes of the largest network that can be reasonably searched. By doing this, I can calculate the Big O value of the algorithm to measure its efficiency. The main output of this project improves the neuroengineer's toolkit of algorithms for modeling networks and discovering the memory-storing polychronous groups within a network. This has many implications, as further studies can utilize this new probing algorithm to design inputs that alter or even implant a desired PNG (memory) in the population. The ability to more efficiently find PNGs in a larger network also allows researchers to explore other cognitive functions related to PNGs only possible in a bigger system such as selective attention to memories. Finally, the ability to control a system of PNGs allows researchers to explore reinforcement learning and stimuli that activate polychronous groups responsible for psychological rewards.

\section{Literature Review}
\hl{
Polychronous groups are a framework for memory storage, but they are a less popular alternative compared to the classical model called a Hopfield network. A visualization for this model is a three-dimensional field with specific memories corresponding to depressions in the field (thus, classification can be performed by tracking where an example "rolls" into). The primary limitation of the network is that memory capacity only increases linearly with neuron count {\cite{hopfield}}. In the PNG paradigm, memories are represented as a series of spikes firing with a specific order and delays (hence the term polychronous){\cite{pngalgo}}. Their combinatorial scaling with the size of the network is the main reason to prefer them over other models for memory storage. The idea was first introduced by Eugene Izhikevich, and in his paper, he uses a collection of simple patterns and randomly searches for PNGs. I am refining his method in this project.\\
A concrete example of a polychronous group is a mouse running a maze {\cite{mouse}}. This is because when the mouse goes along a path, a specific sequence of neurons fire. The same sequences fire again as the mouse recalls the path it took. The act of recall also strengthens the connections between the neurons in the group, due to a phenomenon called spike timing dependent plasticity, solidifying the PNG.\\
My PP-Almost-Seq algorithm builds off of PP-Seq (the Neymann-Scott point process model for sequence detection), which clusters spikes in a raster plot into perceived sequence types {\cite{ppseq}}. The fundamental idea behind PP-Seq is that observations of events center around the true event. My algorithm infers new clusters from the new cluster probabilities calculated within the code of PP-Seq and uses an anti-correlation metric described in previous research on neuron sequences {\cite{correlation}} to find potential PNG candidates that are as different from currently known PNGs as possible. The ability to find polychronous groups is relevant to fields like selective attention to memories, memory-related disorders, and reinforcement learning. Knowing the location and structure of memories allow us to design inputs to strengthen them (like in the case of the mouse) and interpret brain activity.}
\section{Project Plan}
\hl{My} proposed solution for the issue of detecting PNGs in large neural networks is developing a more efficient algorithm. My novel algorithm for discovering polychronous neuronal groups (PNGs) exhibits two main advantages over the existing algorithms \cite{pngalgo}.

Firstly, I take advantage of simulation data to help find PNGs. In Izhikevich's original paper, he did not collect any measurements while the network was being formed and used an algorithm that randomly searched for polychronous groups. I believe that making observations during the development phase of the neuronal population would offer key insights as to where PNGs are located. For instance, a chain of neurons firing subsequently in a network could be a potential indicator of a PNG. Thus, my algorithm marks that group to check if it is actually a polychronous group using the point process model for sequence detection \cite{ppseq}. This method allows us to find a ranked ordering of candidate sequences by likelihood of being real in any simulation.


The second major advantage the algorithm offers over existing PNG-finding algorithms is the consideration of sparseness. For example, if there exists a polychronous group of five neurons firing in the order 1-2-3-4-5, it's unlikely that there's another polychronous group that has much overlap with the first one (in this hypothetical case, a PNG with neurons firing in the order 1-2-3-4-6, differing only in the last neuron). The reason for this is that memories are usually distinct enough such that similar stimuli do not trigger different PNGs. A useful generalization of the deterministic nature of PNGs is that if there are a number of polychronous groups already discovered in a region of a neural network, then it is unlikely that there are many more PNGs within the same area.  algorithm draws on that principle by giving search priority to areas where PNGs have yet to be discovered. I use the Hamming distance as a metric to determine the distance between groups, though I am actively searching for geometric properties and improved metrics for distance throughout the process of research. An analog for this proposed distance advantage is the contrasting strategies of breadth-first-search (BFS) and depth-first-search (DFS) when searching through a tree. One can imagine the root of a tree to be a single neuron, and the children being neurons that receive input from their preceding (parent) neurons. My algorithm would use a method similar to BFS as opposed to DFS (searching widely before searching deeply and primarily changing the last neuron in finding potential groups) due to the general non-overlappingness of PNGs.

\section{Evaluation}
\subsection{Thesis}
The current brute-force algorithm for finding memory-containing PNGs is inaccurate and ineffective for large networks, so I introduce a new algorithm that takes advantage of previous simulation data and the non-overlapping nature of polychronous groups to discover these PNGs.

\subsection{Claim}
My PNG-finding algorithm is more efficient than previous algorithms in terms of Big O value.

\subsection{Evaluation Design}
\subsubsection{Dependent Variable}
The dependent variable that is being compared is the ratio of number of PNGs discovered to the number of measurements needed of the neural population and the Big O values of the algorithms. 

\subsubsection{Independent Variable}
The independent variable is the searching and probing strategy employed by the different algorithms (brute force vs. novel).

\subsubsection{Task}
The task for the two programs is to discover and mark PNGs in a simulated network of Izhikevich neurons.

\subsubsection{Threats}
One threat to the project is the possibility of using simulation parameters that are inaccurate indicators of PNGs, resulting in false positives. Another threat is missing PNGs that do happen to be clustered near other PNGs. Finally, even if this algorithm improves upon the existing one, it is not guaranteed to be efficient enough to handle a large network.

% \subsubsection{Why will this design directly test your thesis?}
% This design directly tests my thesis because the number of measurements a PNG search algorithm must make to find polychronous groups is directly related to how efficient it is, as more efficient algorithms need fewer measurements and thus find PNGs more quickly. 
\section{Resources and Preparation}
Since spring of 2022, I have been studying neurocomputation using a textbook called Dynamical Systems in Neuroscience by Eugene Izhikevich. This work was a part of the introduction to CS research course (CS197) I took that quarter where I prepared for summer research with my mentor, Max Kanwal, a PhD student at the Brains in Silicon Lab. Other CS classes I've taken (CS106B, CS107, CS109) have also strengthened my ability to make good programming decisions and analyze algorithms more effectively. The Math 51 and 53 classes have familiarized me with linear algebra and differential equations respectively, which are essential to understanding the behavior of neuron models.\\
This summer, I prototyped a basic version of the proposed algorithm (called PP-Almost-Seq) while working at the lab and presented my findings at the CURIS poster session. I also participated in weekly lab meetings and group textbook discussions. During the spring and summer, I met with Max once a week either on Zoom or in person. I greatly enjoyed the experience and learning along with my colleagues, which is why I am requesting this grant to continue my research. This is the sole source of funding I am applying to. The final product of my work is to be determined, but I will present this tthis research may contribute towards a thesis for my major and/or a paper for publication.
\section{Timeline}
\hl{
\textbf{Week 1:} Refamiliarize with PP-Almost-Seq code base and add documentation. Sort/truncate the output of the program and make it more readable. \\
\textbf{Week 2:} Create networks with variable number of PNGs and test PP-Almost-Seq's ability to infer PNGs from given ones. Debug if issues arise.\\
\textbf{Week 3:} Continue refining the algorithm to detect larger numbers of PNGs and record the limits of the algorithm.\\
\textbf{Week 4:} Implement a function to determine the most likely order for new sequence candidates based on their usual order of fire amongst neighbors.\\
\textbf{Week 5-6:} Design a script to feed the most likely next candidates back to the network generator to probe and refeed to the PP-Almost-Seq algorithm.\\
\textbf{Week 7:} Debug the online algorithm and measure the performance of the algorithm in finding new PNGs.\\
\textbf{Week 8:} Determine a performance metric. Calculate the performance of the algorithm based on the number of polychronous groups and instances they show up.\\
\textbf{Week 9:} Make a visual of the performance and compare to PP-Seq without modifications and the Izhikevich brute force. Start formalizing results in presentation and/or paper.\\
\textbf{Week 10:} Continue writing presentation and prepare to present it to labmates in lab meeting.
}
\section{Budget}
Stipend: $\$1,500$\\
Total: $\$1,500$\\
The primary software needed for this project (the Brian 2 neural network simulator and PP-Seq source code) is free. The Brains in Silicon Lab also has access to the Sherlock cluster as a computing resource.\\ 
% \input{about.tex} % comment this out before submitting

% BALANCE COLUMNS
\balance{}

% REFERENCES FORMAT
\bibliographystyle{unsrt} % don't remove this line
{\bibliography{bibliography}} % don't remove this line 

\end{document}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
