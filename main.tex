\documentclass{sigchi}
\pagenumbering{arabic}

% Load basic packages
\usepackage{balance}       % to better equalize the last page
\usepackage{graphics}      % for EPS, load graphicx instead 
\usepackage[T1]{fontenc}   % for umlauts and other diaeresis
\usepackage{txfonts}
\usepackage{mathptmx}
\usepackage[pdflang={en-US},pdftex]{hyperref}
\usepackage{color}
\usepackage{booktabs}
\usepackage{textcomp}

% Some optional stuff you might like/need.
\usepackage{microtype}        % Improved Tracking and Kerning
% \usepackage[all]{hypcap}    % Fixes bug in hyperref caption linking
\usepackage{ccicons}          % Cite your images correctly!
% \usepackage[utf8]{inputenc} % for a UTF8 editor only

% If you want to use todo notes, marginpars etc. during creation of
% your draft document, you have to enable the "chi_draft" option for
% the document class. To do this, change the very first line to:
% "\documentclass[chi_draft]{sigchi}". You can then place todo notes
% by using the "\todo{...}"  command. Make sure to disable the draft
% option again before submitting your final document.
\usepackage{todonotes}

% Paper metadata
\def\plaintitle{Do Neural Networks Dream of an Efficient PNG Search Algorithm?}
\def\plainauthor{}
\def\emptyauthor{}
\def\plainkeywords{}
\def\plaingeneralterms{}

% llt: Define a global style for URLs, rather that the default one
\makeatletter
\def\url@leostyle{%
  \@ifundefined{selectfont}{
    \def\UrlFont{\sf}
  }{
    \def\UrlFont{\small\bf\ttfamily}
  }}
\makeatother
\urlstyle{leo}

% To make various LaTeX processors do the right thing with page size.
\def\pprw{8.5in}
\def\pprh{11in}
\special{papersize=\pprw,\pprh}
\setlength{\paperwidth}{\pprw}
\setlength{\paperheight}{\pprh}
\setlength{\pdfpagewidth}{\pprw}
\setlength{\pdfpageheight}{\pprh}

% Make sure hyperref comes last of your loaded packages, to give it a
% fighting chance of not being over-written, since its job is to
% redefine many LaTeX commands.
\definecolor{linkColor}{RGB}{6,125,233}
\hypersetup{%
  pdftitle={\plaintitle},
% Use \plainauthor for final version.
%  pdfauthor={\plainauthor},
  pdfauthor={\emptyauthor},
  pdfkeywords={\plainkeywords},
  pdfdisplaydoctitle=true, % For Accessibility
  bookmarksnumbered,
  pdfstartview={FitH},
  colorlinks,
  citecolor=black,
  filecolor=black,
  linkcolor=black,
  urlcolor=linkColor,
  breaklinks=true,
  hypertexnames=false
}

%% remove the copyright box
\makeatletter
\def\@copyrightspace{\relax}
\makeatother

% create a shortcut to typeset table headings
% \newcommand\tabhead[1]{\small\textbf{#1}}

% End of preamble. Here it comes the document.
\begin{document}

\title{\plaintitle}

\numberofauthors{1}
\author{%
  \alignauthor{Allison Tee\\
    \email{ateecup@stanford.edu}}\\
  }

\maketitle


%%%%%%%%%%%%%%%%%%%% TODO: Fill in these sections
\section{Project Summary}
Polychronous neuronal groups (PNGs) are bunches of neurons in the brain that fire in a specific pattern, and they represent memories. The amount of PNGs in a neural network scales combinatorially with the network size. Because there are more PNGs than neurons, it's hard to find these groups! Previous research in this field employed brute force methods, but they are slow and have limited accuracy. The objective of my research is to develop an efficient PNG-search algorithm that can find more PNGs with fewer measurements. The ability to find polychronous groups is relevant to fields like selective attention to memories, memory-related disorders, and reinforcement learning. I have prototyped an algorithm to find PNGs called "PP-Almost-Seq," which hypothesizes new polychronous groups based on an existing method to find neuron sequences called PP-Seq (short for the Neyman-Scott point process model for sequence detection). It has been tested on a network of 100 Izhikevich neurons simulated using the Brian 2 spiking neural network simulator in Python and works in simple cases with two implanted PNGs in the network. The next steps for my research include automating the loop between the simulation and the PP-Almost-Seq algorithm so that it can constantly detect new groups on its own and implementing a most likely order for candidate PNGs.
\section{Introduction}
By reverse-engineering the brain (one of the 14 grand engineering challenges \cite{grandchal}), it would be possible to build smarter computers that could assist with everyday life, fully understand how the brain makes humans human, and make machines that learn much more quickly and energy efficiently than current deep learning models. The brain is largely made up of neurons, so to achieve this overarching task, one would need to simulate realistic neural networks and develop algorithms to control the dynamics according to parameters for different neuron types. Specifically, the focus of this paper is on "polychronous neuronal groups" (PNGs) that model how a collection of neurons can store a large quantity of memories with each group representing a particular memory \cite{pngintro}. Prominent neurocomputation researcher Eugene Izhikevich discovered that the number of PNGs is far greater than the number of neurons in a network, which causes the system to have an extremely high memory capacity. 


Although the combinatorially increasing number of PNGs allows for vast memory storage, it also causes the current brute-force algorithm for discovering polychronous groups to be extremely inefficient and inaccurate. It does not scale well with large networks, and the algorithm misses PNGs that do not start with a specific three-neuron firing pattern. Other challenges include the fact that probing a PNG rarely activates the entire group, network plasticity causes groups (and consequently memory) to degrade over time, and the network often experiences uncontrolled activity \cite{pngalgo}. 


In this paper, instead of probing randomly like in the original algorithm, we develop a novel, more efficient algorithm to discover PNGs. A smart method to find polychronous groups is needed in order to study memories, especially given how the memory-storing PNGs are more densely populated than the neurons themselves. By developing this new algorithm, we improve the arsenal of existing neuroengineering control algorithms for studying the brain. The first step to develop this solution is to simulate neurons represented by differential equations. Equations with non-linear dynamics are difficult to model because they generally rely on numerical approximations. By using different and more precise numerical methods than previous researchers \cite{simtools}, one could create a more accurate simulation of neural networks. This is especially crucial for this problem because while the original algorithm searched through the neural population, the network was prone to seizures that could be alleviated by fine-tuning and optimizing the performance of the model. Our solution for the problem is to introduce a new and improved probing algorithm to discover polychronous groups. One of the advantages of the algorithm is collecting observations during development to mark likely PNGs in a network. The algorithm will be able to handle searching through larger neural networks and do so more accurately (for instance, identifying PNGs that do not start with three neurons).


To evaluate the programâ€™s ability to find PNGs, we compare how many measurements the algorithm needs of a neural population to discover PNGs relative to the brute-force search algorithm Izhikevich uses and how many PNGs are discovered. We also compare the sizes of the largest network that can be reasonably searched. By doing this, we can calculate the Big O value of the algorithm to measure its efficiency. The main output of this project improves the neuroengineer's toolkit of algorithms for modeling networks and discovering the memory-storing polychronous groups within a network. This has many implications, as further studies can utilize this new probing algorithm to design inputs that alter or even implant a desired PNG (memory) in the population. The ability to more efficiently find PNGs in a larger network also allows researchers to explore other cognitive functions related to PNGs only possible in a bigger system such as selective attention to memories. Finally, the ability to control a system of PNGs allows researchers to explore reinforcement learning and stimuli that activate polychronous groups responsible for psychological rewards.

\section{Related Work}
The subject of neurocomputation and broader field of neuroscience have many unique and varied applications. For example, by learning more about the brain, researchers have been able to solve centuries-old observations such as why the brain perceives light and dark images differently and creates illusions with modern frameworks of neuronal nonlinearity \cite{nonlinearvisual}. The past few decades have brought new advances in neurocomputation and related fields of AI and machine learning: for instance, progress has soared after researchers discovered that one does not necessarily need to understand the algorithms behind how a smart machine thinks, but just how to evolve and learn intelligence, like how humans "naturally" did with evolution \cite{poglevels}. Recent research on deep neural networks also shows that they do not use brute force to memorize data but that they are content aware and take advantage of patterns to learn \cite{memorizationdeep}. However, the main challenge scientists face is that it is difficult to build robots modeled after humans both physically and cognitively because humans are not engineered. The first robot modeling the human upper torso experienced severe movement and wiring issues because there were too many connections from the brain to the rest of the body to integrate all individual parts of the robot to engage with the environment \cite{bodybot}. In the thinking and less movement-oriented regime, there are biologically-inspired bots such as NeuroBot \cite{neurobot} that are programmed using computational neuron models to play first-person shooter games. In a humanness competition with both bot and human participants, Neurobot earned 2nd place with a 36\% humanness rating (albeit, the authors cited massive lag as a obstacle), which was more human than the least human human's 20\%, but only around half the most human human's rating of 67\%. Thus, the current state of biologically-inspired hardware and software is advanced but many challenges prevent robots from being more human.


The design philosophy behind neurocomputation (as opposed to other AI methods) is unique because it hinges on the real-life biological traits of neurons. There are many types of neurons exhibiting extensive behaviors: in the neocortex, the three basic types of firing patterns are regular-spiking, fast-spiking, and intrinsically bursting, with each signal pattern resulting from different signals and resulting in different effects on the brain \cite{firingpatterns}. There are types of neurons such as chattering cells \cite{chattercells} and inhibitory interneurons \cite{twonetworks} that help dictate synchronous behavior in the brain. In the mid 20th-century, researchers discovered that various neuronal states could be represented using differential equations and transformations of periodic functions \cite{parabolicbursting, neuralexcite, hodgkinquantitative}. These discoveries of various properties of neurons have improved our understanding of how the brain functions as a unit as neural interaction and behavior becomes more defined than just a network of electrical signals. Using mathematical tools to describe neurons allows researchers to graph, predict, and simulate behavior. Further ideas to develop on these neuroscience studies is to study the neuronal activity associated with memories, how mental illnesses and differences in personality affect neuronal patterns (and to what magnitude), and experiment with other mathematical branches to see if they can also offer useful frameworks to explain neurons.


In the other, more technological and mathematical approach to studying neuroscience, researchers have created dozens of models for neurons and their features \cite{whichmodel, modelsofsr}, ranging from integrate-and-fire, the simplest model using only 4 floating-point operations (FLOPS) per 1 ms (and one of the worst for simulations), to the extremely detailed and similarly as expensive (at 1200 FLOPS/ms) Hodgkin-Huxley model \cite{hodgkinquantitative} with many equations and parameters. These models fail in large-scale simulation with the first not being nearly biologically accurate enough, and the second being so computationally expensive that only around a dozen Hodgkin-Huxley model neurons can be simulated at once. Thus, researcher Eugene Izhikevich created a model \cite{simplemodel} that is both efficient (13 FLOPS/ms) and biologically accurate, supporting nearly all of the major neurocomputational features. Because Izhikevich's model could be simulated on a larger scale (in tens of thousands of neurons), it is the neuron model selected by researchers to implement the previously mentioned NeuroBot \cite{neurobot}. Other approaches to modeling neurons have also been developed, such as a novel hybrid spiking neuron (HSN) \cite{novelhybrid} that exhibits bifurcations (the quality of a differential equation exhibiting a qualitative change in response to a small change in parameters and key to the behavior of neurons) and can reconstruct characteristics of another neuron with unknown parameters. As opposed to the differential-equation-based integrate-and-fire, Izhikevich, and Hodgkin-Huxley models, the HSN uses discrete dynamics and is suitable for on-chip learning. These models challenged assumptions of previous models as described above, but their general purpose is grounded in describing individual neuron functions rather than a larger neural network or region of the brain. The models also do not speak much about long-term effects on neurons as they are continuously exposed to stimuli. Further potential investigations could focus on emergent properties of neural networks and long-term learning, such as the ability of groups of neurons to store memories, which is the focus of this proposal.\\
\section{Proposed Solution}
Our proposed solution for the issue of detecting PNGs in large neural networks is developing a more efficient algorithm. Our novel algorithm for discovering polychronous neuronal groups (PNGs) exhibits two main advantages over the existing algorithms \cite{pngalgo}.


Firstly, we take advantage of simulation data to help find PNGs. In Izhikevich's original paper, he did not collect any measurements while the network was being formed and used an algorithm that randomly searched for polychronous groups. We believe that making observations during the development phase of the neuronal population would offer key insights as to where PNGs are located. For instance, a chain of neurons firing subsequently in a network could be a potential indicator of a PNG. Thus, our algorithm marks that group to check if it is actually a polychronous group using the point process model for sequence detection \cite{ppseq}. This method allows us to find a ranked ordering of candidate sequences by likelihood of being real in any simulation.


The second major advantage the algorithm offers over existing PNG-finding algorithms is the consideration of sparseness. For example, if there exists a polychronous group of five neurons firing in the order 1-2-3-4-5, it's unlikely that there's another polychronous group that has much overlap with the first one (in this hypothetical case, a PNG with neurons firing in the order 1-2-3-4-6, differing only in the last neuron). The reason for this is that memories are usually distinct enough such that similar stimuli do not trigger different PNGs. A useful generalization of the deterministic nature of PNGs is that if there are a number of polychronous groups already discovered in a region of a neural network, then it is unlikely that there are many more PNGs within the same area. Our algorithm draws on that principle by giving search priority to areas where PNGs have yet to be discovered. We use the Hamming distance as a metric to determine the distance between groups, though we are actively searching for geometric properties and improved metrics for distance throughout the process of research. An analog for this proposed distance advantage is the contrasting strategies of breadth-first-search (BFS) and depth-first-search (DFS) when searching through a tree. One can imagine the root of a tree to be a single neuron, and the children being neurons that receive input from their preceding (parent) neurons. Our algorithm would use a method similar to BFS as opposed to DFS (searching widely before searching deeply and primarily changing the last neuron in finding potential groups) due to the general non-overlappingness of PNGs.

\section{Evaluation Plan}
\subsection{Thesis}
The current brute-force algorithm for finding memory-containing PNGs is inaccurate and ineffective for large networks, so we introduce a smart new algorithm that takes advantage of previous simulation data and the non-overlapping nature of polychronous groups to discover these PNGs.

\subsection{Claim}
Our PNG-finding algorithm is more efficient than previous algorithms in terms of Big O value.

\subsection{Evaluation Design}
\subsubsection{Dependent Variable}
The dependent variable that is being compared is the ratio of number of PNGs discovered to the number of measurements needed of the neural population and the Big O values of the algorithms. 

\subsubsection{Independent Variable}
The independent variable is the searching and probing strategy employed by the different algorithms (brute force vs. novel).

\subsubsection{Task}
The task for the two programs is to discover and mark PNGs in a simulated network of Izhikevich neurons.

\subsubsection{Threats}
One threat to the project is the possibility of using simulation parameters that are inaccurate indicators of PNGs, resulting in false positives. Another threat is missing PNGs that do happen to be clustered near other PNGs. Finally, even if this algorithm improves upon the existing one, it is not guaranteed to be efficient enough to handle a large network.

\subsubsection{Why will this design directly test your thesis?}
This design directly tests my thesis because the number of measurements a PNG search algorithm must make to find polychronous groups is directly related to how efficient it is, as more efficient algorithms need fewer measurements and thus find PNGs more quickly. 
\section{Resources and Preparation}
Since spring of 2022, I have been studying neurocomputation using a textbook called Dynamical Systems in Neuroscience by Eugene Izhikevich. This work was a part of the introduction to CS research course (CS197) I took that quarter where I prepared for summer research with my mentor, Max Kanwal, a PhD student at the Brains in Silicon Lab. Other CS classes I've taken (CS106B, CS107, CS109) have also strengthened my ability to make good programming decisions and analyze algorithms more effectively. The Math 51 and 53 classes have familiarized me with linear algebra and differential equations respectively, which are essential to understanding the behavior of neuron models.\\
This summer, I prototyped a basic version of the proposed algorithm (called PP-Almost-Seq) while working at the lab and presented my findings at the CURIS poster session. I also participated in weekly lab meetings and group textbook discussions. During the spring and summer, I met with Max once a week either on Zoom or in person. I greatly enjoyed the experience and learning along with my colleagues, which is why I am requesting this grant to continue my research. This is the sole source of funding I am applying to. The final product of my work is to be determined, but this research may contribute towards a thesis for my major and/or a paper for publication.
\section{Budget}
Stipend: $\$1,500$\\
Total: $\$1,500$\\
The primary software needed for this project (the Brian 2 neural network simulator and PP-Seq source code) is free. The Brains in Silicon Lab also has access to the Sherlock cluster as a computing resource.\\ 
% \input{about.tex} % comment this out before submitting

% BALANCE COLUMNS
\balance{}

% REFERENCES FORMAT
\bibliographystyle{SIGCHI-Reference-Format} % don't remove this line
\bibliography{bibliography} % don't remove this line 

\end{document}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
